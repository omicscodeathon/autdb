{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba970456-c0be-4bb8-9464-3d315ebfa918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append_engagnition_expand.py\n",
    "# ДЛЯ ENGANITION:\n",
    "# - Создаёт строки уровня session × modality (ACC/GSR/TMP/ENG/GAZE/PERF).\n",
    "# - Для LPE/HPE добавляет строки уровня block там, где в CSV есть Готовый столбец\n",
    "#   c блоками (block/trial/round/task/stage/level/segment/...).\n",
    "# - НИКАКИХ вычислений: только факты (пути, флаги, имена блоков).\n",
    "# - Перед записью удаляет все старые строки с sample_id, начинающимися на \"ENG_\"\n",
    "#   (чтобы избежать дублирования при повторных запусках).\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "CONDITION_DIRS = {\n",
    "    \"Baseline\": \"Baseline condition\",\n",
    "    \"LPE\": \"LPE condition\",\n",
    "    \"HPE\": \"HPE condition\",\n",
    "}\n",
    "\n",
    "# Модальности: имя, имя файла, название \"rel_path_*\", название \"has_*\"\n",
    "MODS = [\n",
    "    (\"ACC\",  \"E4AccData.csv\",        \"rel_path_acc\",         \"has_acc\"),\n",
    "    (\"GSR\",  \"E4GsrData.csv\",        \"rel_path_gsr\",         \"has_gsr\"),\n",
    "    (\"TMP\",  \"E4TmpData.csv\",        \"rel_path_tmp\",         \"has_st\"),\n",
    "    (\"ENG\",  \"EngagementData.csv\",   \"rel_path_engagement\",  \"has_engagement\"),\n",
    "    (\"GAZE\", \"GazeData.csv\",         \"rel_path_gaze\",        \"has_gaze\"),\n",
    "    (\"PERF\", \"PerformanceData.csv\",  \"rel_path_performance\", \"has_performance\"),\n",
    "]\n",
    "\n",
    "def relp(path, start):\n",
    "    return os.path.relpath(path, start=start).replace(\"/\", \"\\\\\")\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return str(s).strip().lower().replace(\" \", \"\").replace(\"_\", \"\")\n",
    "\n",
    "def normalize_condition(s):\n",
    "    t = str(s).strip().lower()\n",
    "    if t in (\"hpe\", \"highphysicalengagement\", \"high-physical-engagement\"):\n",
    "        return \"HPE\"\n",
    "    if t in (\"lpe\", \"lowphysicalengagement\", \"low-physical-engagement\"):\n",
    "        return \"LPE\"\n",
    "    if \"base\" in t:\n",
    "        return \"Baseline\"\n",
    "    return t.upper()\n",
    "\n",
    "# Какие названия колонок считаем \"блоковыми\"\n",
    "BLOCK_KEYS = (\"block\", \"trial\", \"round\", \"task\", \"stage\", \"level\", \"segment\", \"game\")\n",
    "\n",
    "def find_block_column(df: pd.DataFrame):\n",
    "    \"\"\"Пытаемся найти колонку, по которой можно порезать на блоки (2..50 уникальных значений).\n",
    "       Возвращаем (orig_name, uniq_values) или (None, []).\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return None, []\n",
    "    # Сначала пробуем по \"говорящим\" названиям\n",
    "    for c in df.columns:\n",
    "        n = norm(c)\n",
    "        if any(k in n for k in BLOCK_KEYS):\n",
    "            vals = pd.Series(df[c]).dropna().unique().tolist()\n",
    "            if 2 <= len(vals) <= 50:\n",
    "                return c, vals\n",
    "    # Если не нашли — пробуем все категориальные колонки \"в разумных границах\"\n",
    "    for c in df.columns:\n",
    "        vals = pd.Series(df[c]).dropna().unique().tolist()\n",
    "        if 2 <= len(vals) <= 50 and pd.api.types.infer_dtype(vals) in (\"string\", \"mixed\", \"categorical\", \"integer\"):\n",
    "            return c, vals\n",
    "    return None, []\n",
    "\n",
    "def try_read_csv(path):\n",
    "    \"\"\"Читаем CSV 'как есть'. Если не получается — возвращаем None, не падаем.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, low_memory=False)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_rows_for_session_and_blocks(data_root, participant_id, cond, pdir):\n",
    "    \"\"\"Создаём:\n",
    "       - session×modality строки для каждой существующей модальности\n",
    "       - доп. block-строки для модальностей, где удаётся найти 'блоковую' колонку\n",
    "         (обычно PERF и/или GAZE) — ТОЛЬКО для LPE/HPE.\n",
    "    \"\"\"\n",
    "    base_row_common = {\n",
    "        \"dataset\": \"Engagnition\",\n",
    "        \"participant_id\": participant_id,\n",
    "        \"condition\": cond,\n",
    "        \"source_dir\": relp(pdir, data_root),\n",
    "\n",
    "        # цели/таргеты — плейсхолдеры, без вычислений\n",
    "        \"activity_class\": pd.NA,\n",
    "        \"engagement_level\": pd.NA,\n",
    "        \"movement_intensity_raw\": pd.NA,\n",
    "        \"movement_intensity_z\": pd.NA,\n",
    "        \"movement_intensity_bin\": pd.NA,\n",
    "\n",
    "        # совместимость c MMASD\n",
    "        \"activity_prefix\": pd.NA,\n",
    "        \"rel_path_openpose\": pd.NA,\n",
    "\n",
    "        # fairness плейсхолдеры\n",
    "        \"sex\": pd.NA,\n",
    "        \"age_years\": pd.NA,\n",
    "        \"age_group\": pd.NA,\n",
    "\n",
    "        # сплиты плейсхолдеры\n",
    "        \"split_seed\": pd.NA,\n",
    "        \"split_iid\": pd.NA,\n",
    "        \"split_lodo\": pd.NA,\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # ----- 1) session × modality -----\n",
    "    for mod_name, filename, rel_col, has_col in MODS:\n",
    "        fpath = os.path.join(pdir, filename)\n",
    "        exists = os.path.isfile(fpath)\n",
    "\n",
    "        row = base_row_common.copy()\n",
    "        row[\"unit_level\"] = \"session\"\n",
    "        row[\"modality\"] = mod_name\n",
    "        row[\"sample_id\"] = f\"ENG_{participant_id}_{cond}_{mod_name}\"\n",
    "        # заполняем только свой путь + флаг\n",
    "        row[rel_col] = relp(fpath, data_root) if exists else pd.NA\n",
    "        row[has_col] = int(exists)\n",
    "        # остальные rel_path_* оставим пустыми — они не нужны в строке \"чужой\" модальности\n",
    "        rows.append(row)\n",
    "\n",
    "    # ----- 2) block-уровень (только для LPE/HPE) -----\n",
    "    if cond in (\"LPE\", \"HPE\"):\n",
    "        for mod_name, filename, rel_col, has_col in MODS:\n",
    "            if mod_name not in (\"PERF\", \"GAZE\"):\n",
    "                continue  # блоки обычно описаны в PERF/GAZE\n",
    "            fpath = os.path.join(pdir, filename)\n",
    "            if not os.path.isfile(fpath):\n",
    "                continue\n",
    "            df = try_read_csv(fpath)\n",
    "            col, uniq_vals = find_block_column(df)\n",
    "            if not col:\n",
    "                continue\n",
    "\n",
    "            for v in uniq_vals:\n",
    "                brow = base_row_common.copy()\n",
    "                brow[\"unit_level\"] = \"block\"\n",
    "                brow[\"modality\"] = mod_name\n",
    "                # block_id приводим к строке без пробелов\n",
    "                block_id = str(v).strip().replace(\" \", \"\")\n",
    "                brow[\"block_field\"] = col\n",
    "                brow[\"block_id\"] = str(v)\n",
    "                brow[\"sample_id\"] = f\"ENG_{participant_id}_{cond}_{mod_name}_B{block_id}\"\n",
    "                brow[rel_col] = relp(fpath, data_root)\n",
    "                brow[has_col] = 1\n",
    "                rows.append(brow)\n",
    "\n",
    "    return rows\n",
    "\n",
    "def collect_rows(data_root):\n",
    "    eng_root = os.path.join(data_root, \"Engagnition\")\n",
    "    rows = []\n",
    "    for cond, cond_dir in CONDITION_DIRS.items():\n",
    "        base = os.path.join(eng_root, cond_dir)\n",
    "        if not os.path.isdir(base):\n",
    "            continue\n",
    "\n",
    "        for name in sorted(os.listdir(base)):\n",
    "            if not name.upper().startswith(\"P\"):\n",
    "                continue\n",
    "            pdir = os.path.join(base, name)\n",
    "            if not os.path.isdir(pdir):\n",
    "                continue\n",
    "\n",
    "            participant_id = name  # 'Pxx'\n",
    "            rows.extend(build_rows_for_session_and_blocks(data_root, participant_id, cond, pdir))\n",
    "\n",
    "    return pd.DataFrame(rows, dtype=\"object\")\n",
    "\n",
    "# ======== XLSX (готовые значения, без вычислений) ========\n",
    "\n",
    "def find_col(cols_map, predicate):\n",
    "    for orig, n in cols_map.items():\n",
    "        if predicate(n):\n",
    "            return orig\n",
    "    return None\n",
    "\n",
    "def load_intervention_df(xlsx_path):\n",
    "    if not os.path.isfile(xlsx_path):\n",
    "        return pd.DataFrame()\n",
    "    df0 = pd.read_excel(xlsx_path)\n",
    "    if df0.empty:\n",
    "        return pd.DataFrame()\n",
    "    cols_map = {c: norm(c) for c in df0.columns}\n",
    "    pid_col  = find_col(cols_map, lambda n: n.startswith(\"p\") or \"participant\" in n or n in (\"id\",\"pid\"))\n",
    "    cond_col = find_col(cols_map, lambda n: n.startswith(\"condition\"))\n",
    "    type_col = find_col(cols_map, lambda n: \"intervention\" in n and \"type\" in n)\n",
    "    ts_col   = find_col(cols_map, lambda n: \"timestamp\" in n or \"timestamps\" in n or \"time\" in n)\n",
    "    if not pid_col or not cond_col:\n",
    "        return pd.DataFrame()\n",
    "    out = pd.DataFrame()\n",
    "    out[\"participant_id\"] = df0[pid_col].astype(str).str.upper().str.extract(r\"(P\\d+)\")[0]\n",
    "    out[\"condition\"] = df0[cond_col].apply(normalize_condition)\n",
    "    if type_col:\n",
    "        out[\"intervention_type\"] = df0[type_col]\n",
    "    if ts_col:\n",
    "        out[\"intervention_timestamps_raw\"] = df0[ts_col].astype(str)\n",
    "    return out.dropna(subset=[\"participant_id\",\"condition\"]).drop_duplicates([\"participant_id\",\"condition\"])\n",
    "\n",
    "def load_elapsed_df(xlsx_path):\n",
    "    if not os.path.isfile(xlsx_path):\n",
    "        return pd.DataFrame()\n",
    "    df0 = pd.read_excel(xlsx_path)\n",
    "    if df0.empty:\n",
    "        return pd.DataFrame()\n",
    "    cols_map = {c: norm(c) for c in df0.columns}\n",
    "    pid_col  = find_col(cols_map, lambda n: n.startswith(\"p\") or \"participant\" in n or n in (\"id\",\"pid\"))\n",
    "    cond_col = find_col(cols_map, lambda n: n.startswith(\"condition\"))\n",
    "    tot_col  = find_col(cols_map, lambda n: \"totalsec\" in n or \"elapsed\" in n or \"durationsec\" in n or \"totaltime\" in n)\n",
    "    if not pid_col or not cond_col or not tot_col:\n",
    "        return pd.DataFrame()\n",
    "    out = pd.DataFrame()\n",
    "    out[\"participant_id\"] = df0[pid_col].astype(str).str.upper().str.extract(r\"(P\\d+)\")[0]\n",
    "    out[\"condition\"] = df0[cond_col].apply(normalize_condition)\n",
    "    out[\"elapsed_time_sec_total\"] = df0[tot_col]\n",
    "    return out.dropna(subset=[\"participant_id\",\"condition\"]).drop_duplicates([\"participant_id\",\"condition\"])\n",
    "\n",
    "def load_questionnaire_df(xlsx_path):\n",
    "    if not os.path.isfile(xlsx_path):\n",
    "        return pd.DataFrame()\n",
    "    df0 = pd.read_excel(xlsx_path)\n",
    "    if df0.empty:\n",
    "        return pd.DataFrame()\n",
    "    cols_map = {c: norm(c) for c in df0.columns}\n",
    "    pid_col  = find_col(cols_map, lambda n: n.startswith(\"p\") or \"participant\" in n or n in (\"id\",\"pid\"))\n",
    "    cond_col = find_col(cols_map, lambda n: n.startswith(\"condition\"))\n",
    "    sus_col  = find_col(cols_map, lambda n: \"sus\" in n and \"total\" in n)\n",
    "    nasa_w   = find_col(cols_map, lambda n: \"nasa\" in n and \"weighted\" in n)\n",
    "    nasa_u   = find_col(cols_map, lambda n: \"nasa\" in n and (\"unweighted\" in n or \"raw\" in n))\n",
    "    if not pid_col or not cond_col:\n",
    "        return pd.DataFrame()\n",
    "    out = pd.DataFrame()\n",
    "    out[\"participant_id\"] = df0[pid_col].astype(str).str.upper().str.extract(r\"(P\\d+)\")[0]\n",
    "    out[\"condition\"] = df0[cond_col].apply(normalize_condition)\n",
    "    if sus_col:\n",
    "        out[\"sus_total\"] = df0[sus_col]\n",
    "    if nasa_w:\n",
    "        out[\"nasa_tlx_weighted\"] = df0[nasa_w]\n",
    "    if nasa_u:\n",
    "        out[\"nasa_tlx_unweighted\"] = df0[nasa_u]\n",
    "    return out.dropna(subset=[\"participant_id\",\"condition\"]).drop_duplicates([\"participant_id\",\"condition\"])\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--data-root\", required=True)\n",
    "    ap.add_argument(\"--out\", required=True)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    # 1) читаем текущую мету\n",
    "    if os.path.isfile(args.out):\n",
    "        meta = pd.read_csv(args.out, dtype=\"object\")\n",
    "    else:\n",
    "        meta = pd.DataFrame(dtype=\"object\")\n",
    "\n",
    "    # 2) собираем расширенные строки ENGANITION\n",
    "    eng = collect_rows(args.data_root)\n",
    "    if eng.empty:\n",
    "        print(\"[WARN] Engagnition не найден.\")\n",
    "        return\n",
    "\n",
    "    # 3) подмешиваем ГОТОВЫЕ поля из XLSX (если есть)\n",
    "    base = os.path.join(args.data_root, \"Engagnition\")\n",
    "    inter_df   = load_intervention_df(os.path.join(base, \"InterventionData.xlsx\"))\n",
    "    elapsed_df = load_elapsed_df(os.path.join(base, \"Session Elapsed Time.xlsx\"))\n",
    "    quest_df   = load_questionnaire_df(os.path.join(base, \"Subjective questionnaire.xlsx\"))\n",
    "    for extra in (inter_df, elapsed_df, quest_df):\n",
    "        if not extra.empty:\n",
    "            # Мержим по participant_id+condition — данные приклеятся ко всем\n",
    "            # строкам этого участника/условия (и session, и block).\n",
    "            eng = eng.merge(extra, on=[\"participant_id\",\"condition\"], how=\"left\")\n",
    "\n",
    "    # 4) удаляем прежние ENG_* строки, чтобы не плодить дубликаты\n",
    "    if \"sample_id\" in meta.columns:\n",
    "        before = len(meta)\n",
    "        meta = meta[~meta[\"sample_id\"].astype(str).str.startswith(\"ENG_\")]\n",
    "        removed = before - len(meta)\n",
    "        if removed:\n",
    "            print(f\"[INFO] Удалены старые ENG-строки: {removed}\")\n",
    "\n",
    "    # 5) унифицируем колонки и сохраняем\n",
    "    all_cols = list(dict.fromkeys(list(meta.columns) + list(eng.columns)))\n",
    "    meta = meta.reindex(columns=all_cols)\n",
    "    eng  = eng.reindex(columns=all_cols)\n",
    "\n",
    "    out_df = pd.concat([meta, eng], ignore_index=True)\n",
    "    out_df.to_csv(args.out, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[OK] Добавлено ENG-строк: {len(eng)}; всего строк: {len(out_df)}\")\n",
    "    print(f\"[OK] Сохранено: {args.out}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
